---
title: "2025-12-24"
description: "An introduction to Bayesian statistics for neuroscience."
---

<VeslxFrontMatter />

These notes distill the Bayesian workflow used in neuroscience: specify a
generative model, express uncertainty with a prior, and update beliefs using
data. They are based on Chapter 2 of the PhD thesis by James C. R. Whittington,
*A Bayesian Account of Learning and Generalising Representations in the Brain*
(2019) [link](https://ora.ox.ac.uk/objects/uuid:2b513340-9558-41dd-8533-0f250df98c66/files/rwh246s39g).

## From models to likelihood

The probability of observing data $D$ given parameters $\theta$ and model $M$
is

$$
P(D \mid \theta, M)
$$

This is the **likelihood**. It answers: if the parameters were true, how likely
is the data under the model? Because real observations are noisy, we do not
expect perfect fits; the likelihood quantifies how plausible the data are.

A useful quantity is **surprise**, defined as

$$
-\log P(D \mid \theta, M)
$$

Logarithms turn products over independent observations into sums, which are
easier to analyze and compute.

## Maximum likelihood and its limits

We often seek the parameters that make the data most likely:

$$
\theta^* = \arg\max_\theta P(D \mid \theta, M)
$$

Equivalently, this minimizes surprise (negative log-likelihood):

$$
\theta^* = \arg\min_\theta \left[-\log P(D \mid \theta, M)\right]
$$

This procedure is **Maximum Likelihood Estimation (MLE)**. MLE is a point
estimate; it can shift when new data arrive. That motivates treating parameters
as uncertain, not fixed.

## Bayes' rule

Instead of a single estimate, we introduce a **prior** distribution over
parameters:

$$
P(\theta \mid M)
$$

After observing data, we want the **posterior**:

$$
P(\theta \mid D, M)
$$

Bayes' rule connects them:

$$
P(\theta \mid D, M) = \frac{P(D \mid \theta, M) P(\theta \mid M)}{P(D \mid M)}
$$

The denominator $P(D \mid M)$ is the **evidence** or marginal likelihood; it
normalizes the posterior.

## Worked example: fitting a noisy wave

Suppose we observe a noisy sine wave. We treat frequency and phase as known and
infer only the amplitude. The model is:

- $y(t) = A \sin(2\pi f t + \phi) + \epsilon(t)$
- $\theta = A$ (amplitude)
- $M$ = "a sine wave with Gaussian noise"
- $D$ = the recorded data points $\{(t_i, y_i)\}$

The noise term $\epsilon(t)$ captures measurement noise and unmodelled effects.

### Likelihood (derived)

Assume independent Gaussian noise at each sample:

$$
\epsilon_i \sim \mathcal{N}(0, \sigma^2)
$$

Then each observation is distributed as:

$$
y_i \mid A \sim \mathcal{N}\big(A\sin(\omega t_i), \sigma^2\big),
\quad \omega = 2\pi f
$$

Independence across samples gives the joint likelihood:

$$
P(D \mid A, M) = \prod_{i=1}^N \mathcal{N}\big(y_i \mid A\sin(\omega t_i), \sigma^2\big)
$$

Taking the negative log yields a least-squares objective:

$$
-\log P(D \mid A, M) = \text{const} + \frac{1}{2\sigma^2}\sum_i (y_i - A\sin(\omega t_i))^2
$$

### Surprise and MLE for the amplitude

If the amplitude is wrong, the likelihood is small and surprise is large:

$$
\text{surprise}(A) = -\log P(D \mid A, M)
$$

The MLE chooses the amplitude that minimizes this quantity:

$$
A^* = \arg\min_A \big[-\log P(D \mid A, M)\big]
$$

### Prior and posterior for $A$

If we believe extremely large amplitudes are unlikely, we encode that as a
prior $P(A \mid M)$. The posterior combines data and prior:

$$
P(A \mid D, M) \propto P(D \mid A, M) P(A \mid M)
$$

The result is a distribution over plausible amplitudes rather than a single
best value.

## Takeaway

Bayesian modeling makes uncertainty explicit. You specify a generative model,
derive a likelihood, combine it with a prior, and update to a posterior. This
workflow is especially natural in neuroscience, where data are noisy and models
are approximate.
