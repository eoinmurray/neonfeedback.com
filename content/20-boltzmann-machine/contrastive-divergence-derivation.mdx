import { cn } from "@/lib/utils"

<div 
  id="appendix"
  className={cn(
    "flex flex-col md:flex-row justify-between items-center",
    "md:p-12 mx-auto gap-4 text-xs",
    "md:grid md:grid-cols-3"
  )}
>
<div>

<div className="font-semibold font-serif text-4xl">Appendix: Contrastive Divergence</div>

Starting with a Boltzmann machine as defined earlier, we 
want to derive the contrastive divergence algorithm for training.
The goal is to adjust the weights of the network to minimize the energy of the training data.

We have:
- A visible layer $v$ and a hidden layer $h$.
- A weight matrix $W$ that connects the visible and hidden layers.
- A bias vector $b$ for the visible layer and a bias vector $c$ for the hidden layer.

Energy function in matrix form:

$$
E(v,h) = -\sum_{i=1}^{m} \sum_{j=1}^{n} w_{ij} v_i h_j - \sum_{i=1}^{m} b_i v_i - \sum_{j=1}^{n} c_j h_j
$$

Joint distribution:

$$
P(v,h) = \frac{1}{Z} e^{-E(v,h)}
$$

Where $Z$ is the partition function, which normalizes the distribution.

</div>

<div>

We train the RBM by maximizing the likelihood of the training data, i.e. maximizing $\text{log}(P(v))$.

The marginal likelihood of the visible layer is given by:
$$
P(v) = \sum_{h} P(v,h)
$$

Then the log-likelihood is:
$$
\text{log}(P(v)) = \text{log}\sum_{h} \frac{1}{Z} e^{-E(v,h)} = \text{log}\sum_{h} e^{-E(v,h)} - \text{log}(Z)
$$

Differentiating with respect to the weights $w_{ij}$ gives:

$$
\begin{align*}
\frac{\partial \log P(\mathbf{v})}{\partial w_{ij}} 
&= \frac{1}{\sum_{\mathbf{h}} e^{-E(\mathbf{v}, \mathbf{h})}} 
\sum_{\mathbf{h}} \left( -\frac{\partial E(\mathbf{v}, \mathbf{h})}{\partial w_{ij}} \right) e^{-E(\mathbf{v}, \mathbf{h})} \\
&\quad - \frac{1}{Z} \sum_{\mathbf{v}, \mathbf{h}} \left( -\frac{\partial E(\mathbf{v}, \mathbf{h})}{\partial w_{ij}} \right) e^{-E(\mathbf{v}, \mathbf{h})}
\end{align*}
$$

Similar forms exist for the biases $b_i$ and $c_j$.

Since we are performing gradient ascent:

$$
\Delta w_{ij} \leftarrow \Delta w_{ij} + \eta \frac{\partial \log P(v)}{\partial w_{ij}}
$$

Therefore we get our weight update rule:

$$
\Delta w_{ij} = \eta \frac{\partial \log P(v)}{\partial w_{ij}} = \eta \left( \langle v_i h_j \rangle_{data} - \langle v_i h_j \rangle_{model} \right)
$$

</div>
<div>
A similar process can be followed for the biases $b_i$ and $c_j$.
$$
\Delta b_i = \eta \left( \langle v_i \rangle_{data} - \langle v_i \rangle_{model} \right)
$$
$$
\Delta c_j = \eta \left( \langle h_j \rangle_{data} - \langle h_j \rangle_{model} \right)
$$

Where $\langle \cdot \rangle_{data}$ is the expectation with respect to the training data and $\langle \cdot \rangle_{model}$ is the expectation with respect to the model distribution.

The next step is to approximate the model expectation using Gibbs sampling.

1. Positive phase: Sample $\mathbf{h}^{(0)} \approx P(\mathbf{h}|\mathbf{v}^{(0)} = \text{data})$ 
2. Negative phase: Run k steps of Gibbs sampling: 
  - Alternating between $\mathbf{v}^{(t+1)} \approx P(\mathbf{v}|\mathbf{h}^{(t)})$
  - and $\mathbf{h}^{(t+1)} \approx P(\mathbf{h}|\mathbf{v}^{(t)})$

Once those steps are done we update the weights and biases according to:

$$
\Delta w_{ij} = \eta \left( \langle v_i h_j \rangle_{data} - \langle v_i h_j \rangle_{model} \right)
$$
$$
\Delta b_i = \eta \left( \langle v_i \rangle_{data} - \langle v_i \rangle_{model} \right)
$$
$$
\Delta c_j = \eta \left( \langle h_j \rangle_{data} - \langle h_j \rangle_{model} \right)
$$
</div>
</div>